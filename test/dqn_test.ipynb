{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "furnished-sapphire",
   "metadata": {},
   "source": [
    "# Step Wise Test of Framework\n",
    "\n",
    "This notebook is used for peice wise testing of the complete agent framework. Each cell tests limited functionality to specially make sure that tensors have the right dimensions (column / row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "relevant-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as n\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-kinase",
   "metadata": {},
   "source": [
    "## Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advisory-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: (64, 37)\n",
      "actions: (64,)\n",
      "rewards: (64,)\n",
      "next_states = (64, 37)\n",
      "dones = (64,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "state_space = 37\n",
    "action_space = 4\n",
    "batch_size = 64\n",
    "\n",
    "states_np = np.random.rand(batch_size, state_space)\n",
    "actions_np = np.random.randint(action_space, size=batch_size)\n",
    "rewards_np = np.random.rand(batch_size)\n",
    "next_states_np = np.random.rand(batch_size, state_space)\n",
    "dones_np = np.random.randint(2, size=batch_size, dtype = 'bool')\n",
    "\n",
    "print(f'states: {states_np.shape}')\n",
    "print(f'actions: {actions_np.shape}')\n",
    "print(f'rewards: {rewards_np.shape}')\n",
    "print(f'next_states = {next_states_np.shape}')\n",
    "print(f'dones = {dones_np.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-roots",
   "metadata": {},
   "source": [
    "## QNetwork\n",
    "\n",
    "Load the qnetwork from model.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loving-plane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetworks loaded\n"
     ]
    }
   ],
   "source": [
    "from model import QNetwork\n",
    "\n",
    "LR = 5e-4\n",
    "seed = None\n",
    "\n",
    "qtarget_network = QNetwork(state_space, action_space, seed).to(device)\n",
    "qlocal_network = QNetwork(state_space, action_space, seed).to(device)\n",
    "\n",
    "# qtarget is never learnt, always evaluated and copied into\n",
    "qtarget_network.eval()\n",
    "\n",
    "optimizer = optim.Adam(qlocal_network.parameters(), lr = LR)\n",
    "\n",
    "print('QNetworks loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c7d65a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=37, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(state_space, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, action_space),\n",
    "        ).double().to(device)\n",
    "\n",
    "\n",
    "qlocal_network = model()\n",
    "qtarget_network = model()\n",
    "\n",
    "print(qlocal_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "725b4b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1984, device='cuda:0', dtype=torch.float64, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "random_state_np = states_np[np.random.randint(batch_size)]\n",
    "random_state = torch.from_numpy(random_state_np).to(device)\n",
    "actions = model(random_state).max()\n",
    "print(actions)\n",
    "# action_chosen = int(np.argmax(actions.to('cpu').numpy()))\n",
    "\n",
    "# print(f'Action: {actions}')\n",
    "# print(f'Choosen action: {action_chosen}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-creativity",
   "metadata": {},
   "source": [
    "## Confirm Network Definition\n",
    "\n",
    "If one state is passed to it, will it give us the right dimension actions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "radical-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 µs ± 3.69 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "135 µs ± 5.76 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "random_state_np = states_np[np.random.randint(batch_size)]\n",
    "qlocal_network.eval()\n",
    "\n",
    "def numpy_style(random_state):\n",
    "    #random_state = torch.from_numpy(random_state_np).to(device)\n",
    "    actions = qlocal_network(random_state).detach()\n",
    "\n",
    "    # when actual code was run and action was passed to the unity environment it complained about the action\n",
    "    # being int64 so had to be converted\n",
    "\n",
    "    action_chosen = int(np.argmax(actions.to('cpu').numpy()))\n",
    "    return action_chosen\n",
    "\n",
    "def torch_style(random_state):\n",
    "    #random_state = torch.from_numpy(random_state_np).to(device)\n",
    "    action = qlocal_network(random_state).detach().max()\n",
    "    return int(action)\n",
    "\n",
    "\n",
    "random_state = torch.from_numpy(random_state_np).to(device)\n",
    "\n",
    "%timeit numpy_style(random_state)\n",
    "%timeit torch_style(random_state)\n",
    "\n",
    "#print(f'Action: {actions}')\n",
    "#print(f'Choosen action: {action_chosen}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-store",
   "metadata": {},
   "source": [
    "## Convert Numpy Arrays to Tensors\n",
    "\n",
    "Use PyTorch confirm that the value of the actions that were chosen in the batch can be found using qlocal_network\n",
    "\n",
    "Also confirm that **gradient function** is mentioned against all network nodes that require a gradient to be computed\n",
    "\n",
    "**Limit** the batch down to 5 samples so that it is easier to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "prescribed-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_batch = 5\n",
    "\n",
    "states = torch.from_numpy(states_np[:limited_batch]).float().to(device)\n",
    "actions = torch.from_numpy(actions_np[:limited_batch]).long().to(device).unsqueeze(1)\n",
    "rewards = torch.from_numpy(rewards_np[:limited_batch]).float().to(device)\n",
    "next_states = torch.from_numpy(next_states_np[:limited_batch]).float().to(device)\n",
    "dones = torch.from_numpy(dones_np[:limited_batch]).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-course",
   "metadata": {},
   "source": [
    "### First Step: Get Action Values (Using Local Network)\n",
    "\n",
    "First step in DQN is to get the action values using the local network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ordinary-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Network - all action values:\n",
      "tensor([[ 0.0359, -0.0744,  0.0047, -0.1696],\n",
      "        [-0.0039, -0.0258, -0.0824, -0.2261],\n",
      "        [-0.0249, -0.0343, -0.0800, -0.1715],\n",
      "        [ 0.0128, -0.0763, -0.0417, -0.1786],\n",
      "        [-0.0135, -0.0400, -0.0716, -0.2183]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "Actions that were chosen in the batch:\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3]], device='cuda:0')\n",
      "Confirm the value corresponds to the correct action\n",
      " tensor([[ 0.0359],\n",
      "        [-0.0258],\n",
      "        [-0.0249],\n",
      "        [-0.0417],\n",
      "        [-0.2183]], device='cuda:0', grad_fn=<GatherBackward>)\n"
     ]
    }
   ],
   "source": [
    "q_s_local = qlocal_network(states).gather(1, actions)\n",
    "print(f'Local Network - all action values:\\n{qlocal_network(states)}')\n",
    "print(f'Actions that were chosen in the batch:\\n{actions}')\n",
    "print(f'Confirm the value corresponds to the correct action\\n {q_s_local}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-windsor",
   "metadata": {},
   "source": [
    "## Second Step: Compute TD_ERROR\n",
    "\n",
    "For next_state:\n",
    "\n",
    "1) Use the local network to find which action is the best in next_state (the one that has max value)    \n",
    "2) But find and use that particular action's value in target network not local network     \n",
    "3) y = r + γ * QTarget(s', max_a QLocal(s'))    \n",
    "4) Compute td_error = y - q(s,a)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-stanford",
   "metadata": {},
   "source": [
    "### Confirm QLocal(next_state) and the Maximum Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "assigned-characterization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0310, -0.0276, -0.0750, -0.1856],\n",
      "        [ 0.0005, -0.0472, -0.0428, -0.1894],\n",
      "        [-0.0500,  0.0090, -0.1007, -0.2008],\n",
      "        [-0.0096,  0.0020, -0.0830, -0.1934],\n",
      "        [-0.0093, -0.0511, -0.0646, -0.1829]], device='cuda:0')\n",
      "torch.return_types.max(\n",
      "values=tensor([-0.0276,  0.0005,  0.0090,  0.0020, -0.0093], device='cuda:0'),\n",
      "indices=tensor([1, 0, 1, 1, 0], device='cuda:0'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc1): Linear(in_features=37, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output): Linear(in_features=32, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlocal_netowrk.eval()\n",
    "\n",
    "q_s_prime_local = qlocal_network(next_states).detach()\n",
    "print(q_s_prime_local)\n",
    "print(torch.max(q_s_prime_local, axis = 1))\n",
    "\n",
    "qlocal_netowrk.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-palmer",
   "metadata": {},
   "source": [
    "### Complete TD_Error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "unauthorized-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = r + 𝛾 * max_a V(s')\n",
      "r: tensor([0.0134, 0.3142, 0.6854, 0.3508, 0.9808], device='cuda:0')\n",
      "Actions: tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [3]], device='cuda:0')\n",
      "Q(s' target): tensor([[-0.0365,  0.1683,  0.0832, -0.0310],\n",
      "        [-0.0444,  0.1373,  0.0966,  0.0100],\n",
      "        [-0.0098,  0.1504,  0.0865,  0.0382],\n",
      "        [-0.0524,  0.1961,  0.0484,  0.0316],\n",
      "        [-0.0114,  0.1336,  0.0442, -0.0370]], device='cuda:0')\n",
      "Q(s' local): tensor([[-0.0310, -0.0276, -0.0750, -0.1856],\n",
      "        [ 0.0005, -0.0472, -0.0428, -0.1894],\n",
      "        [-0.0500,  0.0090, -0.1007, -0.2008],\n",
      "        [-0.0096,  0.0020, -0.0830, -0.1934],\n",
      "        [-0.0093, -0.0511, -0.0646, -0.1829]], device='cuda:0')\n",
      "A(s' local): tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], device='cuda:0')\n",
      "QTarget(s', max_a QLocal(s')):\n",
      "tensor([[ 0.1683],\n",
      "        [-0.0444],\n",
      "        [ 0.1504],\n",
      "        [ 0.1961],\n",
      "        [-0.0114]], device='cuda:0')\n",
      "future_rewards: gamma * max_a V(s'):\n",
      "tensor([[0.1666],\n",
      "        [-0.0000],\n",
      "        [0.1489],\n",
      "        [0.1942],\n",
      "        [-0.0000]], device='cuda:0')\n",
      "rewards: tensor([0.0134, 0.3142, 0.6854, 0.3508, 0.9808], device='cuda:0')\n",
      "y = r + future: tensor([[0.1800],\n",
      "        [0.3142],\n",
      "        [0.8344],\n",
      "        [0.5450],\n",
      "        [0.9808]], device='cuda:0')\n",
      "q_s_local: tensor([[ 0.0359],\n",
      "        [-0.0258],\n",
      "        [-0.0249],\n",
      "        [-0.0417],\n",
      "        [-0.2183]], device='cuda:0', grad_fn=<GatherBackward>)\n",
      "td_error: [0.14413764 0.33993885 0.8592455  0.58665264 1.1991776 ]\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "debug = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    q_s_prime_target = qtarget_network(next_states).detach()\n",
    "\n",
    "    # put the local network in eval mode so that in future if it has dropout layers etc.\n",
    "    # they are used in eval\n",
    "    qlocal_netowrk.eval()\n",
    "    q_s_prime_local = qlocal_network(next_states).detach()\n",
    "    qlocal_netowrk.train()\n",
    "\n",
    "    a_s_prime_local = torch.max(q_s_prime_local, axis = 1).indices.unsqueeze(1)\n",
    "    v_s_prime_target = q_s_prime_target.gather(1, a_s_prime_local)\n",
    "\n",
    "    # delta = r + (1-terminal) * gamma * max_a Q(s2, a) - Q(s, a)\n",
    "    # dones is a row vector, we need to convert it to column vector to multiply\n",
    "    # it with gamma * max_a.values\n",
    "    dones_row = (1 - dones).unsqueeze(1)\n",
    "    future_rewards = dones_row * GAMMA * v_s_prime_target\n",
    "\n",
    "    y = rewards.unsqueeze(1) + future_rewards\n",
    "\n",
    "    td_error = (y - q_s_local).to('cpu').numpy().reshape(-1)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"y = r + 𝛾 * max_a V(s')\")\n",
    "        print(f'r: {rewards}')\n",
    "        print(f'Actions: {actions}')\n",
    "        print(f\"Q(s' target): {q_s_prime_target}\")\n",
    "        print(f\"Q(s' local): {q_s_prime_local}\")\n",
    "        print(f\"A(s' local): {a_s_prime_local}\")\n",
    "        print(f\"QTarget(s', max_a QLocal(s')):\\n{v_s_prime_target}\")\n",
    "        print(f\"future_rewards: gamma * max_a V(s'):\\n{future_rewards}\")\n",
    "        print(f\"rewards: {rewards}\")\n",
    "        print(f\"y = r + future: {y}\")\n",
    "        print(f'q_s_local: {q_s_local}')\n",
    "        print(f'td_error: {td_error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-shaft",
   "metadata": {},
   "source": [
    "### Reason for Unsqueezing\n",
    "\n",
    "In the following cell, one can see that v_s_prime_target is a column vector, where each entry in the vector depicts the value of the maximum action for next_state (the index of which was found using the local network NOT target network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "private-ethics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Dones & v_s_prime_target Vectors --------------------\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]], device='cuda:0')\n",
      "tensor([[ 0.1683],\n",
      "        [-0.0444],\n",
      "        [ 0.1504],\n",
      "        [ 0.1961],\n",
      "        [-0.0114]], device='cuda:0')\n",
      "-------------------- Future Rewards --------------------\n",
      "tensor([[0.1666],\n",
      "        [-0.0000],\n",
      "        [0.1489],\n",
      "        [0.1942],\n",
      "        [-0.0000]], device='cuda:0')\n",
      "-------------------- Immediate Rewards (r) --------------------\n",
      "tensor([[0.0134],\n",
      "        [0.3142],\n",
      "        [0.6854],\n",
      "        [0.3508],\n",
      "        [0.9808]], device='cuda:0')\n",
      "-------------------- r + Immediate Rewards --------------------\n",
      "tensor([[0.1800],\n",
      "        [0.3142],\n",
      "        [0.8344],\n",
      "        [0.5450],\n",
      "        [0.9808]], device='cuda:0')\n",
      "-------------------- td_error --------------------\n",
      "tensor([[0.1441],\n",
      "        [0.3399],\n",
      "        [0.8592],\n",
      "        [0.5867],\n",
      "        [1.1992]], device='cuda:0')\n",
      "-------------------- td_error in row format similar to indices --------------------\n",
      "[0.14413764 0.33993885 0.8592455  0.58665264 1.1991776 ]\n"
     ]
    }
   ],
   "source": [
    "def print_heading(heading):\n",
    "    print('-' * 20, end = '')\n",
    "    print(f' {heading} ', end ='')\n",
    "    print('-' * 20)\n",
    "\n",
    "dones_row = (1 - dones).unsqueeze(1)\n",
    "print_heading('Dones & v_s_prime_target Vectors')\n",
    "print(dones_row)\n",
    "print(v_s_prime_target)\n",
    "\n",
    "future_rewards = dones_row * GAMMA * v_s_prime_target\n",
    "print_heading('Future Rewards')\n",
    "print(future_rewards)\n",
    "\n",
    "print_heading('Immediate Rewards (r)')\n",
    "print(rewards.unsqueeze(1))\n",
    "\n",
    "y = rewards.unsqueeze(1) + future_rewards\n",
    "print_heading('r + Immediate Rewards')\n",
    "print(y)\n",
    "\n",
    "td_error = (y - q_s_local).detach()\n",
    "print_heading('td_error')\n",
    "print(td_error)\n",
    "\n",
    "print_heading('td_error in row format similar to indices')\n",
    "print(td_error.data.to('cpu').numpy().reshape(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-craps",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "`y - q(s,a)` has to be multiplied by the weights returned by prioritized replay buffer therefore we cannot use MSELoss as is and will have to compute Squared Error first, then multiply by weights and then take the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fifty-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[0.1800],\n",
      "        [0.3142],\n",
      "        [0.8344],\n",
      "        [0.5450],\n",
      "        [0.9808]], device='cuda:0')\n",
      "q_s_local: tensor([[ 0.0359],\n",
      "        [-0.0258],\n",
      "        [-0.0249],\n",
      "        [-0.0417],\n",
      "        [-0.2183]], device='cuda:0', grad_fn=<GatherBackward>)\n",
      "td_error = y - q_s_local = tensor([[0.1441],\n",
      "        [0.3399],\n",
      "        [0.8592],\n",
      "        [0.5867],\n",
      "        [1.1992]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Confirmation that reduction=None in the loss function gives us just (y - Q(s)) ** 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "td_error ** 2: tensor([[0.0208],\n",
      "        [0.1156],\n",
      "        [0.7383],\n",
      "        [0.3442],\n",
      "        [1.4380]], device='cuda:0', grad_fn=<PowBackward0>)\n",
      "loss: tensor([[0.0208],\n",
      "        [0.1156],\n",
      "        [0.7383],\n",
      "        [0.3442],\n",
      "        [1.4380]], device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mean loss: 0.5313650369644165\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "td_error = y - q_s_local\n",
    "\n",
    "optimizer = optim.Adam(qlocal_netowrk.parameters(), lr = LR)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# loss function should only compute (x - y) ** 2 and not mean it\n",
    "# loss = F.mse_loss(y, q_s_local) \n",
    "loss_fn = nn.MSELoss(reduction='none')\n",
    "loss = loss_fn(y, q_s_local)\n",
    "loss_mean = torch.mean(loss)\n",
    "loss_mean.backward()\n",
    "\n",
    "print(f'y: {y}')\n",
    "print(f'q_s_local: {q_s_local}')\n",
    "print(f'td_error = y - q_s_local = {td_error}')\n",
    "print('-' * 100)\n",
    "print('Confirmation that reduction=None in the loss function gives us just (y - Q(s)) ** 2')\n",
    "print('-' * 100)\n",
    "print(f'td_error ** 2: {td_error ** 2}')\n",
    "print(f'loss: {loss}')\n",
    "print('-' * 100)\n",
    "print(f'Mean loss: {loss_mean}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
